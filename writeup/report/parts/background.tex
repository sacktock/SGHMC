%!TEX root = ../report.tex

\section{Background}

\subsection{HMC}

Hamiltonian Monte Carlo (\cite{duane-hmc,neal-hmc}) is a Markov Chain Monte Carlo (MCMC) sampling algorithm. Given a target probability distribution — in our case the posterior distribution of a set of variables $\theta$ given independent observations $x \in \D$ — it produces samples by carrying out a random walk over the parameter space using Hamiltonian dynamics.

We begin with the prior distribution $p(\theta)$ and likelihood $p(x \mid \theta)$. Using these we define the \emph{potential energy} function $U$:
\begin{equation*}
    U(\theta) \defeq - \sum_{x \in \D}\log p(x \mid \theta) - \log p(\theta)
\end{equation*}
Note that, using Bayes' rule, we have that the posterior $p(\theta \mid \D) \propto \exp(-U)$. Hamiltonian dynamics introduces an auxiliary set of momentum variables $r$. These dynamics have a physical interpretation in which an object moves about a landscape determined by $U$. We let this object have \emph{mass matrix} $M$, which is typically set to the identity matrix. Then $U(\theta)$ represents the potential energy of the object, and its kinetic energy is given by $\frac 1 2 r^{\mathsf T} M^{-1} r$. The total energy of the system is a quantity known as the \emph{Hamiltonian function}:
\begin{equation*}
    H(\theta, r) = U(\theta) + \frac 1 2 r^{\mathsf T} M^{-1} r
\end{equation*}
The development of the system is governed by the following equations.
\begin{align*}
    \dif\theta &= M^{-1}r \dif t \\
    \dif r &= - \nabla U(\theta) \dif t
\end{align*}

To simulate these continuous dynamics in practice, we must use a discretised version of these equations. To correct for the inaccuracies introduced by doing so, it is necessary to make a \emph{Metropolis-Hastings correction step}. A simple algorithm is given in \cref{alg:hmc}.

\begin{algorithm}
    \caption{A simple HMC algorithm}\label{alg:hmc}
    \begin{algorithmic}
        \For{$t = 1,2, \ldots$}
            \State $r \sim \mc N(0,1)$ \Comment{Resample momentum}
            \State $(\theta_0, r_0) = (\theta, r)$ 
            \For{$i = 1$ to $m$}
                \State $\theta \gets \theta + \ep M^{-1}r$
                \State $r \gets r - \ep \nabla U(\theta)$
            \EndFor
            \State $u \sim \rm{Uniform}[0,1]$
            \State $\rho = \exp(H(\theta, r) - H(\theta_0, r_0))$ \Comment{Acceptance probability}
            \If{$u < \min(1, \rho)$} \Comment{Only accept new state with probability $\rho$}
                \State $\theta = \theta_0$
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}

In practice, the dataset $\D$ may be large, and so running \cref{alg:hmc} may be computationally expensive, as the complexity of calculating $\nabla U$ scales with $\abs\D$. One idea to combat this is to simulate the Hamiltonian system using only a subset of the data at a time, in analogy with stochastic gradient descent. This method is known as Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), however before explaining the SGHMC algorithm we begin with Naïve SGHMC.


\subsection{Naïve SGHMC}

To understand the Naïve SGHMC method, consider sampling a minibatch $\wt\D \subset \D$ uniformly at random. We estimate the gradient $\nabla U(\theta)$ using this minibatch as follows:
\begin{equation*}
    \nabla \wt U(\theta) = - \frac{\abs\D}{\abs{\wt\D}}\sum_{x \in \wt\D} \nabla \log p(x \mid \theta) - \nabla \log p(\theta)
\end{equation*}
Appealing to the Central Limit Theorem, we imagine that the noisy estimate $\nabla \wt U(\theta)$ is normally distributed about $\nabla U(\theta)$, with some covariance matrix $V(\theta)$. The naïve adaptation of HMC to this stochastic scenario simply replaces $\nabla U(\theta)$ with $\nabla \wt U(\theta)$ in \cref{alg:hmc}. The corresponding discrete system is then the $\ep$-discretisation of the following dynamics:
\begin{align*}
    \dif\theta &= M^{-1}r \dif t \\
    \dif r &= - \nabla U(\theta) \dif t + \mc N(0, 2B(\theta) \dif t)
\end{align*}
where $B(\theta) = \frac 1 2 \ep V(\theta)$.

Unfortunately, such a dynamical system can diverge quite rapidly from the true posterior distribution \cite{neal-hmc}, which necessitates frequent Metropolis-Hastings steps. Such steps are costly since calculating the acceptance probability requires the use of the whole dataset. The method `Stochastic Gradient Hamiltonian Monte Carlo' (SGHMC) proposed in \cite{sghmc} addresses this shortcoming. The idea is to incorporate friction into the dynamical system, which works to counteract the noise introduced by selecting a subset of the data.


\subsection{SGHMC}

The SGHMC method adds a `friction term' $B M^{-1} r$ to the momentum update. Since we are unlikely to know the noise model $B$ in practice, we instead take an estimate $\wh B$ of $B$, together with a user-specified friction term $C \succeq \wh B$ and simulate the following dynamics.
\begin{align*}
    \dif\theta &= M^{-1}r \dif t \\
    \dif r &= - \nabla U(\theta) \dif t - CM^{-1}r \dif t + \mc N(0, 2(C - \wh B(\theta))\dif t) + \mc N(0, 2B(\theta) \dif t)
\end{align*}
The algorithm is given in \cref{alg:sghmc}. Ideally we would have $\wh B = B$, allowing us to reach the posterior distribution in an efficient manner. In practice, we must rely on an inaccurate estimate $\wh B$. The simplest choice is $\wh B = 0$. A better but more costly estimate is $\wh B(\theta) = \frac 1 2 \ep \wh V(\theta)$, where $\wh V$ is the observed (empirical Fisher) information \cite{sgld-fisher}. The friction term $C$ can then be taken as a hyperparameter, and set so as to counteract the inaccuracies of the estimate $\wh B$.

\begin{algorithm}
    \caption{The SGHMC algorithm}\label{alg:sghmc}
    \begin{algorithmic}
        \For{$t = 1,2, \ldots$}
            \State $r \sim \mc N(0,1)$ \Comment{Resample momentum}
            \For{$i = 1$ to $m$}
                \State $\theta \gets \theta + \ep M^{-1}r$
                \State $r \gets r - \ep \nabla \wt U(\theta) - \ep CM^{-1}r + \mc N(0, 2(C- \wh B(\theta))\ep)$
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}