%!TEX root = ../report.tex

\section{Conclusion}

For this project, we have implemented a number of algorithms for the purpose of replicating and extending the experiments in \cite{sghmc}. We opted for a close integration with Pyro which allowed us to make use of its probabilistic programming framework. 

Replicating the analysis in \cite{sghmc} we compared the performance of SGHMC with related samplers. We confirmed their results that on the toy models SGHMC samples accurately from the posterior and efficiently samples from correlated distributions. Next, we tested SGHMC on a BNN for classifying MNIST digits, comparing it to SGLD, SGD and SGD with momentum. We obtained similar results to the original experiment. Moreover, we expanded the analysis by testing SGMHC on a BNN for the Fashion-MNIST dataset and a CNN for CIFAR10. In both cases SGMHC performed very well. In the former we further compared SGLD, SGD and SGD with momentum, with SGMHC producing the superior performance. Given the close integration of our codebase with Pyro, it is easy to extend to further models and datasets, which we would do if given more time.

As a further extension, we briefly compared SGHMC with the more popular variational inference method, using our BNN on the MNIST dataset. Over a large number of epochs SGHMC performs better than VI. However, the latter converges faster, and requires fewer samples to give a representative picture of the posterior. With more time, we would like to investigate this further, for example by comparing the sampling efficiency more in-depth.

Our implementation of SGHMC provided the option of estimating the noise model using the observed information, as suggested in \cite{sghmc}. Given more time, we would like to investigate alternative methods of estimating the noise model, ideally ones less computationally expensive.

Our last extension to \cite{sghmc} was the specification and implementation of our new algorithm SGNUTS, which combines SGHMC with the No-U-Turn Sampler. In spite of the lack of reversibility of the corresponding Markov process, our algorithm performs well on a BNN for the MNIST and Fashion-MNIST datasets. We found that while SGNUTS takes longer than SGHMC to produce a single sample, but reaches high accuracies in a relatively small number of steps. If we were to continue this project, we would investigate in more detail the theoretical and practical aspects of SGNUTS.