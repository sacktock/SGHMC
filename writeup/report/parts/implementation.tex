
%!TEX root = ../report.tex

\section{Implementation Details}

We implemented the following algorithms from scratch: HMC, SGHMC, SGLD (stochastic gradient Langevin dynamics \cite{sgld}), SGD (stochastic gradient descent), SGD with Nesterov momentum and SGNUTS (stochastic gradient No U-Turn Sampler) — a novel extension to SGHMC that uses ideas from the popular No U-Turn Sampler \cite{nuts}. All of our implementations subclass Pyro's \texttt{MCMCKernel} and are designed to be used directly with Pyro \cite{pyro} — a universal probabilistic programming language (PPL) written in Python. In Pyro the user specifies a model which is a probabilistic program (PP) that describes a posterior distribution $p(\theta \: | \: \D)$ that we want to sample from; $\theta$ corresponds to the sampled parameters or latent parameters of the model and $\D$ corresponds to the observed data. 

Pyro already comes with the following MCMC samplers: HMC, MH and NUTS. So while the algorithms we implemented are not novel they are in fact innovative since Pyro doesn't come with any of them already built. The main reason for choosing to implement our algorithms on top of Pyro is because Pyro comes with a method \texttt{initialize\_model} that given a Pyro PP or model $P$ transforms it into a potential function $U$. Once we have $U$ we can pass the latent and observed data $(\theta, \D)$ to $U$, which computes the negative log joint $- \log p(\theta, \D)$. Bayes' rule tells us that  $p(\theta \: | \: \D) \propto p(\theta, \D)$ which is typically all we need for MCMC samplers and even simpler ones such as Importance and Rejection samplers \cite{bishop2006pattern}. Pyro also comes with the method \texttt{potential\_grad}, which given $(\theta, \D)$ computes the gradient of $U$ with respect to the parameters $\theta$. The result is that we can specify any arbitrary PP and apply our algorithms to them — letting Pyro handle the transformation from PP to potential function and the gradient computations.

In traditional MCMC samplers the observed dataset $\D$ is constant and so once a Pyro PP has been transformed into a potential function $U(\theta) = - \log p(\theta, \D)$ we need not change it. Unfortunately this is less straight forward for stochastic gradient samplers such as SGHMC and SGLD since we subsample the full dataset $\D$ by sampling minibatches $\wt{\D}$, where $\wt{\D} \subset \D$. In both SGHMC and SGLD we require that the potential function has the form:
$$\wt{U}(\theta) =  -\frac{|\D|}{|\wt{\D}|} \log p(\wt{\D} \: | \: \theta) - \log p (\theta)$$
Unfortunately when we subsample the dataset and call \texttt{initialize\_model} Pyro doesn't explicitly supply us with the likelihood term $p(\wt{\D} \: | \: \theta)$ - it only gives us the negative log joint $- \log p(\theta, \wt{\D})$, so to get the desired behaviour above we had to get our hands dirty and modify Pyro's source code. As a result every time we generate a new sample using SGHMC or SGLD we have to call \texttt{initialize\_model} so that it gives us the correct $\wt{U}(\theta)$ for some given minibatch $\wt{\D}$, although this is a small price to pay for much quicker gradient computations.

For the estimate of $\wh B$, in our implementation we provide two options: take $\wh B = 0$ or use $\wh B(\theta) = \frac 1 2 \ep \wh V(\theta)$, where $\wh V$ is the observed information, as suggested in \cite{sghmc}. While the latter provides a better estimate, it is much slower, and requires more memory. When $\wh B$ is estimated using the observed information, we provided the option to compute it once at setup time, recalculate every sample, or recalculate every step when simulating the dynamics.