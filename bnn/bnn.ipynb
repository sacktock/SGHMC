{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns # conda install seaborn\n",
    "import pandas as pd # ^^ this will automatically install pandas\n",
    "\n",
    "import pyro\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from kernel.sghmc import SGHMC\n",
    "from kernel.sgld import SGLD\n",
    "from kernel.sgd import SGD\n",
    "from kernel.sgnuts import NUTS as SGNUTS\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b6cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset wrapper class\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf72cc",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 800\n",
    "WARMUP_EPOCHS = 50\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9339186",
   "metadata": {},
   "source": [
    "### Download MNIST and setup datasets / dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f96971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "nvalid = 10000\n",
    "\n",
    "perm = torch.arange(len(train_dataset))\n",
    "train_idx = perm[nvalid:]\n",
    "val_idx = perm[:nvalid]\n",
    "    \n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# scale the datasets\n",
    "X_train = train_dataset.data[train_idx] / 255.0\n",
    "Y_train = train_dataset.targets[train_idx]\n",
    "\n",
    "X_val = train_dataset.data[val_idx] / 255.0\n",
    "Y_val = train_dataset.targets[val_idx]\n",
    "\n",
    "X_test = test_dataset.data / 255.0\n",
    "Y_test = test_dataset.targets\n",
    "\n",
    "# redefine the datasets\n",
    "train_dataset = Dataset(X_train, Y_train)\n",
    "val_dataset = Dataset(X_val, Y_val)\n",
    "test_dataset = Dataset(X_test, Y_test)\n",
    "\n",
    "# setup the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25885bb0",
   "metadata": {},
   "source": [
    "### Define the Bayesian neural network  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroLinear = pyro.nn.PyroModule[torch.nn.Linear]\n",
    "    \n",
    "class BNN(pyro.nn.PyroModule):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, prec=1.):\n",
    "        super().__init__()\n",
    "        # prec is a kwarg that should only used by SGD to set the regularization strength \n",
    "        # recall that a Guassian prior over the weights is equivalent to L2 norm regularization in the non-Bayes setting\n",
    "        \n",
    "        # TODO add gamma priors to precision terms\n",
    "        self.fc1 = PyroLinear(input_size, hidden_size)\n",
    "        self.fc1.weight = pyro.nn.PyroSample(dist.Normal(0., prec).expand([hidden_size, input_size]).to_event(2))\n",
    "        self.fc1.bias   = pyro.nn.PyroSample(dist.Normal(0., prec).expand([hidden_size]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroLinear(hidden_size, output_size)\n",
    "        self.fc2.weight = pyro.nn.PyroSample(dist.Normal(0., prec).expand([output_size, hidden_size]).to_event(2))\n",
    "        self.fc2.bias   = pyro.nn.PyroSample(dist.Normal(0., prec).expand([output_size]).to_event(1))\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)# output (log) softmax probabilities of each class\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=x), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b5534",
   "metadata": {},
   "source": [
    "### Run SGHMC \n",
    "\n",
    "We run SGHMC to sample approximately from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 100\n",
    "NUM_STEPS = 1\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sghmc_test_errs = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sghmc_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828292c",
   "metadata": {},
   "source": [
    "### Run SGLD\n",
    "\n",
    "We run SGLD to sample approximately from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 4e-5\n",
    "LR_DECAY = True\n",
    "\n",
    "if LR_DECAY:\n",
    "    D = 0.25 # decay by 1/4\n",
    "    B = (NUM_EPOCHS * D**2) / (1 - D**2)\n",
    "    A = LR * np.sqrt((NUM_EPOCHS * D**2) / (1 - D**2))\n",
    "    \n",
    "NUM_STEPS = 1\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sgld = SGLD(bnn,\n",
    "            subsample_positions=[0, 1],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LR_START,\n",
    "            num_steps=NUM_STEPS)\n",
    "\n",
    "sgld_mcmc = MCMC(sgld, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgld_test_errs = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):   \n",
    "    sgld_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        if LR_DECAY:\n",
    "            LR = A / np.sqrt((B + (epoch-1)))\n",
    "            sgld_mcmc.kernel.learning_rate = LR\n",
    "        \n",
    "        start = time.time()\n",
    "        sgld_samples = sgld_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sgld_samples)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "            \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                if LR_DECAY:\n",
    "                    predictive_one_hot = predictive_one_hot * LR\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgld_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fac32",
   "metadata": {},
   "source": [
    "### Run SGD\n",
    "\n",
    "We run SGD to optimise the weights of the BNN and we take a point estimate which is the most recent sample to be our \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 6e-5\n",
    "WEIGHT_DECAY=0.0\n",
    "WITH_MOMENTUM=False\n",
    "MOMENTUM_DECAY=1.0\n",
    "REGULARIZATION_TERM=1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, prec=REGULARIZATION_TERM)\n",
    "\n",
    "sgd = SGD(bnn,\n",
    "          subsample_positions=[0, 1],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          learning_rate=LR,\n",
    "          weight_decay=WEIGHT_DECAY,\n",
    "          with_momentum=WITH_MOMENTUM,\n",
    "          momentum_decay=MOMENTUM_DECAY)\n",
    "\n",
    "sgd_mcmc = MCMC(sgd, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgd_test_errs = []\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS+WARMUP_EPOCHS):\n",
    "    sgd_mcmc.run(X_train, Y_train)\n",
    "        \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgd_samples = sgd_mcmc.get_samples()\n",
    "        point_estimate = {site : sgd_samples[site][-1, :].unsqueeze(0) for site in sgd_samples.keys()}\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=point_estimate)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x, y in val_loader:\n",
    "                batch_predictive = predictive(x)['obs']\n",
    "                batch_y_hat = batch_predictive.mode(0)[0]\n",
    "                total += y.shape[0]\n",
    "                correct += int((batch_y_hat == y).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgd_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053562e4",
   "metadata": {},
   "source": [
    "### Run SGD with momentum\n",
    "\n",
    "We run SGD with momentum to optimise the weights of the BNN and we take a point estimate which is the most recent sample to be our \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 8e-4\n",
    "WEIGHT_DECAY=1e-6\n",
    "WITH_MOMENTUM=True\n",
    "MOMENTUM_DECAY=0.01\n",
    "REGULARIZATION_TERM=1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, prec=REGULARIZATION_TERM)\n",
    "\n",
    "sgdmom = SGD(bnn,\n",
    "             subsample_positions=[0, 1],\n",
    "             batch_size=BATCH_SIZE,\n",
    "             learning_rate=LR,\n",
    "             weight_decay=WEIGHT_DECAY,\n",
    "             with_momentum=WITH_MOMENTUM,\n",
    "             momentum_decay=MOMENTUM_DECAY)\n",
    "\n",
    "sgdmom_mcmc = MCMC(sgdmom, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgdmom_test_errs = []\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS+WARMUP_EPOCHS):\n",
    "    sgdmom_mcmc.run(X_train, Y_train)\n",
    "        \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgdmom_samples = sgdmom_mcmc.get_samples()\n",
    "        point_estimate = {site : sgdmom_samples[site][-1, :].unsqueeze(0) for site in sgdmom_samples.keys()}\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=point_estimate)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x, y in val_loader:\n",
    "                batch_predictive = predictive(x)['obs']\n",
    "                batch_y_hat = batch_predictive.mode(0)[0]\n",
    "                total += y.shape[0]\n",
    "                correct += int((batch_y_hat == y).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgdmom_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a3e7f",
   "metadata": {},
   "source": [
    "### Plot the convergence dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aed042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "    \n",
    "sghmc_test_errs = np.array(sghmc_test_errs)\n",
    "sgld_test_errs = np.array(sgld_test_errs)\n",
    "sgd_test_errs = np.array(sgd_test_errs)\n",
    "sgdmom_test_errs = np.array(sgdmom_test_errs)\n",
    "\n",
    "err_dict = {'SGHMC' : sghmc_test_errs, 'SGLD' : sgld_test_errs, 'SGD' : sgd_test_errs, 'SGD with momentum' : sgdmom_test_errs}\n",
    "x = np.arange(1, NUM_EPOCHS+1)\n",
    "lst = []\n",
    "for i in range(len(x)):\n",
    "    for updater in err_dict.keys():\n",
    "        lst.append([x[i], updater, err_dict[updater][i]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['iterations', 'updater','test error'])\n",
    "sns.lineplot(data=df.pivot(\"iterations\", \"updater\", \"test error\"))\n",
    "plt.ylabel(\"test error\")\n",
    "plt.show() #dpi=300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e0df1",
   "metadata": {},
   "source": [
    "Stochastic Gradient NUTS - experimental doesn't quite work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f463b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "NUM_STEPS = 1\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sgnuts = SGNUTS(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              with_friction=True,\n",
    "              obs_info_noise=False)\n",
    "\n",
    "sgnuts_mcmc = MCMC(sgnuts, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    sgnuts_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgnuts_samples = sgnuts_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sgnuts_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int8)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int8)), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
